{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f53bea-fefb-478f-9ba0-640fe7437e3e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98a70a4a-16f5-4b0c-b81d-8959032ae55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and text handling\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from near_regex import NEAR_regex\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Gathering 2022 returns\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "\n",
    "# Get filing dates from SEC EDGAR\n",
    "from requests_html import HTMLSession\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abfadaba-7f99-4f53-8278-18682dc448ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and file handling\n",
    "input_dir = 'inputs'\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Inputs\n",
    "topic_path = input_dir + '/topic_list.csv'\n",
    "sp500_path = input_dir + '/s&p500_2022.csv'\n",
    "firm_10k_path = \"10k_files/sec-edgar-filings\"\n",
    "\n",
    "# Outputs\n",
    "sentiment_save_path = output_dir + '/ticker_sentiments.csv'\n",
    "returns_save_path = output_dir + '/ticker_returns.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55228d9-c6cb-42ce-be39-b03ea1955fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Load S&P500 companies into a dataframe\n",
    "# if not os.path.exists(sp500_path):\n",
    "#     url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "#     pd.read_html(url)[0].to_csv(sp500_path, index=False)  # [1] shows updates\n",
    "\n",
    "# sp500 = pd.read_csv(sp500_path)[['Symbol', 'Security', 'CIK']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b45f5f-e4de-4d83-8ae1-014115d1b1b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load 2022 returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db8388d-726a-4d46-aeb5-01c7e2700566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>-0.011276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-02</td>\n",
       "      <td>0.030954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>0.000287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>0.014362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>0.012459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594044</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>-0.017551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594045</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>-0.114089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594046</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>0.033089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594047</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>0.080827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594048</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>0.011164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2594049 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ticker       date       ret\n",
       "0         JJSF 2021-12-01 -0.011276\n",
       "1         JJSF 2021-12-02  0.030954\n",
       "2         JJSF 2021-12-03  0.000287\n",
       "3         JJSF 2021-12-06  0.014362\n",
       "4         JJSF 2021-12-07  0.012459\n",
       "...        ...        ...       ...\n",
       "2594044   TSLA 2022-12-23 -0.017551\n",
       "2594045   TSLA 2022-12-27 -0.114089\n",
       "2594046   TSLA 2022-12-28  0.033089\n",
       "2594047   TSLA 2022-12-29  0.080827\n",
       "2594048   TSLA 2022-12-30  0.011164\n",
       "\n",
       "[2594049 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download 2022 CSRP returns\n",
    "url = \"https://github.com/LeDataSciFi/data/raw/main/Stock%20Returns%20(CRSP)/crsp_2022_only.zip\"\n",
    "with urlopen(url) as request:\n",
    "    data = BytesIO(request.read())\n",
    "\n",
    "with ZipFile(data) as archive:\n",
    "    with archive.open(archive.namelist()[0]) as stata:\n",
    "        stock_rets = pd.read_stata(stata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069649c-2d84-4ab3-9f3b-4896eaec6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7279d-13d3-48b5-8d54-2bc08ba66f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** TODO make this generic at top for both\n",
    "# Get df of companies\n",
    "if not os.path.exists(sp500_path):\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    pd.read_html(url)[0].to_csv(sp500_path, index=False)  # [1] shows updates\n",
    "\n",
    "sp500 = pd.read_csv(sp500_path)[['Symbol', 'Security', 'CIK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb63301-4fa8-4f8d-94a5-a06e6671f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filing date for each 10-K\n",
    "session = HTMLSession()\n",
    "\n",
    "for i in tqdm(range(len(sp500))):  # TODO:\n",
    "    tic = sp500['Symbol'].iloc[i]\n",
    "    cik = sp500['CIK'].iloc[i]\n",
    "    \n",
    "    if not os.path.exists(fr'{firm_10k_path}/{tic}/10-K/'):\n",
    "        print(f'Error finding accession number for {tic}')\n",
    "        continue\n",
    "    accession = os.listdir(fr'{firm_10k_path}/{tic}/10-K/')[0]\n",
    "    \n",
    "    url = f'https://www.sec.gov/Archives/edgar/data/{cik}/{accession}-index.html'\n",
    "    r = session.get(url)\n",
    "    sp500.loc[i, 'filing_date'] = r.html.find('.info', first=True).text\n",
    "\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9717040-92bd-47cf-abbe-fd5dc8f6d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on filing date, add return from t to t+2 and from t+3 to t+10\n",
    "combined_return = sp500.merge(\n",
    "        stock_rets.rename(columns={'ticker':'Symbol'}),\n",
    "        on='Symbol',\n",
    "        how='left',\n",
    "        validate='1:m') # TODO: include more?\n",
    "combined_return = combined_return.query('filing_date <= date').groupby('Symbol').head(11)\n",
    "combined_return['agg_ret'] = 1 + combined_return['ret']\n",
    "\n",
    "combined_return['prod2'] = combined_return.groupby('Symbol').head(3)['agg_ret'].cumprod() - 1\n",
    "combined_return['prod10'] = combined_return.groupby('Symbol').tail(8)['agg_ret'].cumprod() - 1\n",
    "final_return = combined_return.groupby('Symbol').head(3).groupby('Symbol').tail(1)[['Symbol', 'Security', 'CIK', 'filing_date', 'prod2']]\n",
    "final_return = final_return.merge(\n",
    "        combined_return.groupby('Symbol').tail(1)[['Symbol', 'Security', 'CIK', 'prod10']],\n",
    "        on=['Symbol', 'Security', 'CIK'],\n",
    "        validate='1:1',\n",
    "        how='left') # TODO: include more?\n",
    "\n",
    "final_return.to_csv(returns_save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d419bcd-216c-4760-a4fe-180b89c7c21f",
   "metadata": {},
   "source": [
    "## Load sentiment dictionaries\n",
    "\n",
    "TODO: justify that positive values as of 2021 are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8025a5e8-5384-4cc7-87ac-0ebdad9212af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Dictionaries\n",
    "with open('inputs/ML_negative_unigram.txt', 'r') as file:\n",
    "    BHR_negative = [line.strip() for line in file]\n",
    "with open('inputs/ML_positive_unigram.txt', 'r') as file:\n",
    "    BHR_positive = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8936ac5-2136-4730-8a72-c582d7d5c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM Dictionaries\n",
    "LM = pd.read_csv('inputs/LM_MasterDictionary_1993-2021.csv')\n",
    "LM_negative = LM.query('Negative > 0')['Word'].to_list()\n",
    "LM_positive = LM.query('Positive > 0')['Word'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc85e2a-0ec6-4231-bafd-a00dc9c71ec1",
   "metadata": {},
   "source": [
    "## Load each firm and add sentiment variables\n",
    "\n",
    "TODO: load and clean\n",
    "\n",
    "For each firm, \n",
    "\n",
    "- [ ] load the corresponding 10-K. Clean the text.\n",
    "\n",
    "- [ ] Create the sentiment measurements, and save those new measurements to the correct row and column in the dataframe.\n",
    "\n",
    "- [ ] Bonus: Save the total length of the document (# of words)\n",
    "\n",
    "- [ ] Bonus: Save the # of unique words (similar to total length)\n",
    "\n",
    "- [ ] Calculate returns from t to t+2 inclusive\n",
    "\n",
    "- [ ] Calculate returns from t+3 to t+10 inclusive\n",
    "\n",
    "- [ ] Download 2021 accounting data (2021 ccm_cleaned.dta) from the data repo (possibly useful in analysis) and add to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab6e2a7-47f6-446d-8dbf-558eeb76bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather sentiments into regex\n",
    "BHR_negative_regex = '(' + '|'.join(BHR_negative).lower() + ')'\n",
    "BHR_positive_regex = '(' + '|'.join(BHR_positive).lower() + ')'\n",
    "LM_negative_regex = '(' + '|'.join(LM_negative).lower() + ')'\n",
    "LM_positive_regex = '(' + '|'.join(LM_positive).lower() + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ecbb34-6075-4967-9f6a-45afece8fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Topic regex\n",
    "# def NEAR_regex_helper(topic_list, valence_list, max_words_between=5):\n",
    "#     topic_regex = '(' + '|'.join(topic_list).lower() + ')'\n",
    "#     valence_regex = '(' + '|'.join(valence_list).lower() + ')'\n",
    "#     return NEAR_regex([topic_regex, valence_regex], max_words_between=max_words_between)\n",
    "\n",
    "# # Socially responsible investing\n",
    "# esg_topics = ['esg', 'sustainable', 'sustainability', 'impact invest',\n",
    "#               'clean energy', 'gender', 'diversity', 'inclusion',\n",
    "#               'microfinance', 'ethical', 'cdfi']\n",
    "# esg_negative = ['limited', 'limit', 'underdeveloped', 'underdevelop',\n",
    "#                 'bureaucratic', 'slow', 'insufficient']\n",
    "# esg_positive = ['ethical', 'sustainable', 'profitable', 'profit',\n",
    "#                 'innovative', 'innovation', 'transformative', 'transform']\n",
    "# esg_negative_regex = NEAR_regex_helper(esg_topics, esg_negative)\n",
    "# esg_positive_regex = NEAR_regex_helper(esg_topics, esg_positive)\n",
    "\n",
    "# # Ecommerce\n",
    "# ecom_topics = ['online', 'digital payment', 'logistics', 'delivery', \n",
    "#                'mobile commerce', 'social commerce', 'dropship',\n",
    "#                'drop ship', 'social media']\n",
    "# ecom_negative = ['risky', 'unsustainable',  'unsustained', 'monopoly',\n",
    "#                  'monopolistic', 'unethical']\n",
    "# ecom_positive = ['convenienent', 'convenienence', 'accessible', 'access',\n",
    "#                  'innovative', 'innovation', 'profitable', 'profit',\n",
    "#                  'efficient']\n",
    "# ecom_negative_regex = NEAR_regex_helper(ecom_topics, ecom_negative)\n",
    "# ecom_positive_regex = NEAR_regex_helper(ecom_topics, ecom_positive)\n",
    "\n",
    "# # Biotech and healthcare\n",
    "# bio_topics = ['gene', 'biopharm', 'telemedic', 'personalized medic',\n",
    "#               'medical device', 'vaccine', 'precision medic', 'organ',\n",
    "#               'regenerative medic', 'prosthetic', 'clinic', 'fda',\n",
    "#               'health']\n",
    "# bio_negative = ['risky', 'expensive', 'slow', 'controversial', 'unethical']\n",
    "# bio_positive = ['new', 'safe', 'innovative', 'innovation',\n",
    "#                 'transformative', 'transform', 'life', 'lives']\n",
    "# bio_negative_regex = NEAR_regex_helper(bio_topics, bio_negative)\n",
    "# bio_positive_regex = NEAR_regex_helper(bio_topics, bio_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "201d2cd9-291f-4bd2-bfb8-c3fe6e393b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic regex\n",
    "def NEAR_regex_helper(topic_list, valence_list, max_words_between=5):\n",
    "    topic_regex = '(' + '|'.join(topic_list).lower() + ')'\n",
    "    valence_regex = '(' + '|'.join(valence_list).lower() + ')'\n",
    "    return NEAR_regex([topic_regex, valence_regex], max_words_between=max_words_between)\n",
    "\n",
    "# Read in file\n",
    "if not os.path.exists(topic_path):\n",
    "    print(f'Cannot find path {topic_path} to topic list')\n",
    "else:\n",
    "    topic_df = pd.read_csv(topic_path)\n",
    "\n",
    "    # ESG\n",
    "    esg_topic = topic_df['term'].loc[topic_df['type'] == 'esg_topic']\n",
    "    esg_negative = topic_df['term'].loc[topic_df['type'] == 'esg_negative']\n",
    "    esg_positive = topic_df['term'].loc[topic_df['type'] == 'esg_positive']\n",
    "\n",
    "    # Ecommerce\n",
    "    ecom_topic = topic_df['term'].loc[topic_df['type'] == 'ecom_topic']\n",
    "    ecom_negative = topic_df['term'].loc[topic_df['type'] == 'ecom_negative']\n",
    "    ecom_positive = topic_df['term'].loc[topic_df['type'] == 'ecom_positive']\n",
    "\n",
    "    # Bio\n",
    "    bio_topic = topic_df['term'].loc[topic_df['type'] == 'bio_topic']\n",
    "    bio_negative = topic_df['term'].loc[topic_df['type'] == 'bio_negative']\n",
    "    bio_positive = topic_df['term'].loc[topic_df['type'] == 'bio_positive']\n",
    "\n",
    "    # Generate regex\n",
    "    esg_negative_regex = NEAR_regex_helper(esg_topic, esg_negative)\n",
    "    esg_positive_regex = NEAR_regex_helper(esg_topic, esg_positive)\n",
    "    ecom_negative_regex = NEAR_regex_helper(ecom_topic, ecom_negative)\n",
    "    ecom_positive_regex = NEAR_regex_helper(ecom_topic, ecom_positive)\n",
    "    bio_negative_regex = NEAR_regex_helper(bio_topic, bio_negative)\n",
    "    bio_positive_regex = NEAR_regex_helper(bio_topic, bio_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1a8b55e-82d0-4270-a13e-d4dec142060c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?:\\\\b(esg|sustainable|sustained|sustainability|impact investing|impact invest|clean energy|clean investing|clean invest|diversity|equity|inclusion|microfinance|ethics|ethically|cdfi)\\\\b(?: +[^ \\\\n]*){0,5} *\\\\b(limit|limited|underdevelop|underdeveloped|bureaucratic|slow|slowly|insufficient|behind|poor|faulty|behind)\\\\b)|(?:\\\\b(limit|limited|underdevelop|underdeveloped|bureaucratic|slow|slowly|insufficient|behind|poor|faulty|behind)\\\\b(?: +[^ \\\\n]*){0,5} *\\\\b(esg|sustainable|sustained|sustainability|impact investing|impact invest|clean energy|clean investing|clean invest|diversity|equity|inclusion|microfinance|ethics|ethically|cdfi)\\\\b)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esg_negative_regex\n",
    "# # esg_positive_regex\n",
    "# # ecom_negative_regex\n",
    "# # ecom_positive_regex\n",
    "# # bio_negative_regex\n",
    "# # bio_positive_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be15445f-5c9e-40a6-be0a-16015841d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove duplicated code here\n",
    "if not os.path.exists(sp500_path):\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    pd.read_html(url)[0].to_csv(sp500_path, index=False)  # [1] shows updates\n",
    "\n",
    "sp500 = pd.read_csv(sp500_path)[['Symbol', 'Security', 'CIK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4b9eb13-c23d-4798-98d2-0fb2720d8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify \n",
    "def add_sentiment(df, i, sentiment_name, search, text, word_count):\n",
    "    df.loc[i, sentiment_name] = len(re.findall(search, text)) / word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81201d43-c23b-4601-bade-b3eadad933cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████▊                                 | 64/503 [14:43<1:42:36, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker BRK.B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████                                | 80/503 [18:04<1:37:50, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker BF.B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▍                        | 169/503 [38:07<1:17:33, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker ELV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████▍                      | 197/503 [45:40<1:35:45, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker FRC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████████▋                     | 213/503 [48:56<1:04:28, 13.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker GEHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████▋      | 417/503 [1:34:26<20:13, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker SBNY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████▍ | 481/503 [1:48:44<04:46, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker WBD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 503/503 [1:53:13<00:00, 13.51s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>CIK</th>\n",
       "      <th>word_count</th>\n",
       "      <th>bhr_negative</th>\n",
       "      <th>bhr_positive</th>\n",
       "      <th>lm_negative</th>\n",
       "      <th>lm_positive</th>\n",
       "      <th>esg_negative</th>\n",
       "      <th>esg_positive</th>\n",
       "      <th>ecom_negative</th>\n",
       "      <th>ecom_positive</th>\n",
       "      <th>bio_negative</th>\n",
       "      <th>bio_positive</th>\n",
       "      <th>unique_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>66740</td>\n",
       "      <td>76432.0</td>\n",
       "      <td>0.044314</td>\n",
       "      <td>0.041750</td>\n",
       "      <td>0.043097</td>\n",
       "      <td>0.015125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>6384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>91142</td>\n",
       "      <td>33810.0</td>\n",
       "      <td>0.032919</td>\n",
       "      <td>0.038391</td>\n",
       "      <td>0.034102</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>3801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>1800</td>\n",
       "      <td>52061.0</td>\n",
       "      <td>0.039492</td>\n",
       "      <td>0.039761</td>\n",
       "      <td>0.036246</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>5045.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>1551152</td>\n",
       "      <td>61560.0</td>\n",
       "      <td>0.035185</td>\n",
       "      <td>0.034958</td>\n",
       "      <td>0.035819</td>\n",
       "      <td>0.014766</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>5886.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>1467373</td>\n",
       "      <td>51953.0</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>0.046754</td>\n",
       "      <td>0.036437</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>5169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>YUM</td>\n",
       "      <td>Yum! Brands</td>\n",
       "      <td>1041061</td>\n",
       "      <td>84234.0</td>\n",
       "      <td>0.032873</td>\n",
       "      <td>0.041052</td>\n",
       "      <td>0.034630</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>4945.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>ZBRA</td>\n",
       "      <td>Zebra Technologies</td>\n",
       "      <td>877212</td>\n",
       "      <td>46978.0</td>\n",
       "      <td>0.038273</td>\n",
       "      <td>0.044680</td>\n",
       "      <td>0.033846</td>\n",
       "      <td>0.019775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ZBH</td>\n",
       "      <td>Zimmer Biomet</td>\n",
       "      <td>1136869</td>\n",
       "      <td>64913.0</td>\n",
       "      <td>0.038544</td>\n",
       "      <td>0.035232</td>\n",
       "      <td>0.039176</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>4577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>ZION</td>\n",
       "      <td>Zions Bancorporation</td>\n",
       "      <td>109380</td>\n",
       "      <td>61897.0</td>\n",
       "      <td>0.029307</td>\n",
       "      <td>0.028208</td>\n",
       "      <td>0.028951</td>\n",
       "      <td>0.008756</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3818.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis</td>\n",
       "      <td>1555280</td>\n",
       "      <td>78429.0</td>\n",
       "      <td>0.044512</td>\n",
       "      <td>0.035561</td>\n",
       "      <td>0.038073</td>\n",
       "      <td>0.014472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>5913.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol              Security      CIK  word_count  bhr_negative  \\\n",
       "0      MMM                    3M    66740     76432.0      0.044314   \n",
       "1      AOS           A. O. Smith    91142     33810.0      0.032919   \n",
       "2      ABT                Abbott     1800     52061.0      0.039492   \n",
       "3     ABBV                AbbVie  1551152     61560.0      0.035185   \n",
       "4      ACN             Accenture  1467373     51953.0      0.034127   \n",
       "..     ...                   ...      ...         ...           ...   \n",
       "498    YUM           Yum! Brands  1041061     84234.0      0.032873   \n",
       "499   ZBRA    Zebra Technologies   877212     46978.0      0.038273   \n",
       "500    ZBH         Zimmer Biomet  1136869     64913.0      0.038544   \n",
       "501   ZION  Zions Bancorporation   109380     61897.0      0.029307   \n",
       "502    ZTS                Zoetis  1555280     78429.0      0.044512   \n",
       "\n",
       "     bhr_positive  lm_negative  lm_positive  esg_negative  esg_positive  \\\n",
       "0        0.041750     0.043097     0.015125      0.000000      0.000013   \n",
       "1        0.038391     0.034102     0.013162      0.000030      0.000000   \n",
       "2        0.039761     0.036246     0.010584      0.000019      0.000000   \n",
       "3        0.034958     0.035819     0.014766      0.000016      0.000016   \n",
       "4        0.046754     0.036437     0.018440      0.000000      0.000058   \n",
       "..            ...          ...          ...           ...           ...   \n",
       "498      0.041052     0.034630     0.013771      0.000059      0.000000   \n",
       "499      0.044680     0.033846     0.019775      0.000000      0.000000   \n",
       "500      0.035232     0.039176     0.013233      0.000015      0.000031   \n",
       "501      0.028208     0.028951     0.008756      0.000016      0.000032   \n",
       "502      0.035561     0.038073     0.014472      0.000000      0.000051   \n",
       "\n",
       "     ecom_negative  ecom_positive  bio_negative  bio_positive  \\\n",
       "0         0.000000       0.000000      0.000052      0.000118   \n",
       "1         0.000000       0.000000      0.000000      0.000089   \n",
       "2         0.000000       0.000038      0.000115      0.000365   \n",
       "3         0.000000       0.000016      0.000065      0.000276   \n",
       "4         0.000019       0.000038      0.000000      0.000058   \n",
       "..             ...            ...           ...           ...   \n",
       "498       0.000024       0.000059      0.000012      0.000024   \n",
       "499       0.000000       0.000000      0.000000      0.000000   \n",
       "500       0.000000       0.000000      0.000031      0.000169   \n",
       "501       0.000000       0.000000      0.000000      0.000000   \n",
       "502       0.000000       0.000013      0.000077      0.000255   \n",
       "\n",
       "     unique_word_count  \n",
       "0               6384.0  \n",
       "1               3801.0  \n",
       "2               5045.0  \n",
       "3               5886.0  \n",
       "4               5169.0  \n",
       "..                 ...  \n",
       "498             4945.0  \n",
       "499             4750.0  \n",
       "500             4577.0  \n",
       "501             3818.0  \n",
       "502             5913.0  \n",
       "\n",
       "[503 rows x 15 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tqdm(range(len(sp500))):\n",
    "    tic = sp500['Symbol'].iloc[i]\n",
    "    \n",
    "    # Check existence of path\n",
    "    if not os.path.exists(fr'{firm_10k_path}/{tic}'):\n",
    "        print(f'Cannot find 10-K for ticker {tic}')\n",
    "        continue\n",
    "    \n",
    "    for path in glob.glob(fr'{firm_10k_path}/{tic}/*/*/*.html'):\n",
    "        # Open and clean the 10-K\n",
    "        with open(path, 'rb') as report_file:\n",
    "            html = report_file.read()\n",
    "        soup = BeautifulSoup(html, 'lxml-xml')\n",
    "        for div in soup.find_all(\"div\", {'style': 'display:none'}):\n",
    "            div.decompose()                       # remove hidden divs,\n",
    "        lower = soup.get_text().lower()           # uppercase,\n",
    "        no_punc = re.sub(r'\\W', ' ', lower)       # non-alpha-numeric,\n",
    "        cleaned = re.sub(r'\\s+', ' ', no_punc)    # single-space\n",
    "    \n",
    "        # Add word count and unique word count\n",
    "        word_list = re.findall(r'\\w+', cleaned)\n",
    "        sp500.loc[i, 'unique_word_count'] = len(set(word_list))\n",
    "        word_count = len(word_list)\n",
    "        sp500.loc[i, 'word_count'] = word_count\n",
    "        \n",
    "        # Gather valence variables\n",
    "        add_sentiment(sp500, i, 'bhr_negative', BHR_negative_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'bhr_positive', BHR_positive_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'lm_negative', LM_negative_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'lm_positive', LM_positive_regex, cleaned, word_count)\n",
    "        \n",
    "        # Gather topic valence variables\n",
    "        add_sentiment(sp500, i, 'esg_negative', esg_negative_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'esg_positive', esg_positive_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'ecom_negative', ecom_negative_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'ecom_positive', ecom_positive_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'bio_negative', bio_negative_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'bio_positive', bio_positive_regex, cleaned, word_count)\n",
    "        \n",
    "        # Save intermittently\n",
    "        if i % 25 == 0:\n",
    "            sp500.to_csv(sentiment_save_path, index=False)\n",
    "\n",
    "sp500.to_csv(sentiment_save_path, index=False)\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89391a-1bf8-40a1-88b7-6b2efe8e9cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4685d86b-4e36-4871-aa3a-0f0a11942775",
   "metadata": {},
   "source": [
    "## Add 10-K sentiment data\n",
    "\n",
    "TODO: load and clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29691e2-acc7-4588-bb85-0bd161b71d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f481ec-45af-40e1-a139-e4b4800607ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100815c-41e1-405a-8a0f-e4d2964233f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
