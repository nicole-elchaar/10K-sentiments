{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51300186-a82a-4637-83d5-26c6467c1fa6",
   "metadata": {},
   "source": [
    "# Build Sample\n",
    "\n",
    "After running *download_text_files.ipynb*, we can now build the sample of data to use in the analysis.  The main output of this file is *output/analysis_sample.csv* which includes the following:\n",
    "1. 5 positive sentiment scores\n",
    "1. 5 negative sentiment scores\n",
    "1. a return variable from the day of the 10-K's release to two days following\n",
    "1. a return variable from two days after the 10-K's release to ten days following \n",
    "1. the 10-K's word count\n",
    "1. the 10-K's unique word count\n",
    "1. accounting variables from the CCM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f53bea-fefb-478f-9ba0-640fe7437e3e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a70a4a-16f5-4b0c-b81d-8959032ae55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and text handling\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from near_regex import NEAR_regex\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Gathering 2022 returns\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "\n",
    "# Get filing dates from SEC EDGAR\n",
    "from requests_html import HTMLSession\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abfadaba-7f99-4f53-8278-18682dc448ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and file handling\n",
    "input_dir = 'inputs'\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Inputs\n",
    "topic_path = input_dir + '/topic_list.csv'\n",
    "sp500_path = input_dir + '/s&p500_2022.csv'\n",
    "firm_10k_path = '10k_files/sec-edgar-filings'\n",
    "firm_10k_clean_path = '10k_files/clean'\n",
    "\n",
    "# Outputs\n",
    "sentiment_save_path = output_dir + '/ticker_sentiments_temp.csv'    # store intermediate results\n",
    "returns_save_path = output_dir + '/ticker_returns_temp.csv'         # store intermediate results\n",
    "final_save_path = output_dir + '/analysis_sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77672b37-cd40-4d2a-afd8-e1bd2b8ec9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>CIK</th>\n",
       "      <th>truth_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>66740</td>\n",
       "      <td>MMM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>91142</td>\n",
       "      <td>AOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>1800</td>\n",
       "      <td>ABT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>1551152</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>1467373</td>\n",
       "      <td>ACN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>YUM</td>\n",
       "      <td>Yum! Brands</td>\n",
       "      <td>1041061</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>ZBRA</td>\n",
       "      <td>Zebra Technologies</td>\n",
       "      <td>877212</td>\n",
       "      <td>ZBRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ZBH</td>\n",
       "      <td>Zimmer Biomet</td>\n",
       "      <td>1136869</td>\n",
       "      <td>ZBH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>ZION</td>\n",
       "      <td>Zions Bancorporation</td>\n",
       "      <td>109380</td>\n",
       "      <td>0000109380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis</td>\n",
       "      <td>1555280</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol              Security      CIK  truth_path\n",
       "0      MMM                    3M    66740         MMM\n",
       "1      AOS           A. O. Smith    91142         AOS\n",
       "2      ABT                Abbott     1800         ABT\n",
       "3     ABBV                AbbVie  1551152        ABBV\n",
       "4      ACN             Accenture  1467373         ACN\n",
       "..     ...                   ...      ...         ...\n",
       "498    YUM           Yum! Brands  1041061         YUM\n",
       "499   ZBRA    Zebra Technologies   877212        ZBRA\n",
       "500    ZBH         Zimmer Biomet  1136869         ZBH\n",
       "501   ZION  Zions Bancorporation   109380  0000109380\n",
       "502    ZTS                Zoetis  1555280         ZTS\n",
       "\n",
       "[503 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load S&P500 companies into a dataframe\n",
    "try:\n",
    "    sp500_orig = pd.read_csv(sp500_path)[['Symbol', 'Security', 'CIK', 'truth_path']]\n",
    "except Exception as error:\n",
    "    print('Please run the contents of download_text_files.ipynb before proceeding')\n",
    "    print(repr(error))\n",
    "    \n",
    "sp500_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b45f5f-e4de-4d83-8ae1-014115d1b1b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load 2022 returns\n",
    "\n",
    "Because this step is not costly compared to others, we gather the filing dates regardless of whether we have done so before.\n",
    "\n",
    "Once we have the filing dates, we extract only the 11 days following the announcement.  We then can compute returns for each period of days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb63301-4fa8-4f8d-94a5-a06e6671f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████▎                                                | 197/503 [01:01<01:40,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error finding accession number for ticker FRC, cik 1132979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████▉                                              | 213/503 [01:05<01:29,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error finding accession number for ticker GEHC, cik 1932393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████▎             | 417/503 [02:06<00:23,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error finding accession number for ticker SBNY, cik 1288784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 503/503 [02:30<00:00,  3.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>CIK</th>\n",
       "      <th>truth_path</th>\n",
       "      <th>filing_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>66740</td>\n",
       "      <td>MMM</td>\n",
       "      <td>2022-02-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>91142</td>\n",
       "      <td>AOS</td>\n",
       "      <td>2022-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>1800</td>\n",
       "      <td>ABT</td>\n",
       "      <td>2022-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>1551152</td>\n",
       "      <td>ABBV</td>\n",
       "      <td>2022-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>1467373</td>\n",
       "      <td>ACN</td>\n",
       "      <td>2022-10-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>YUM</td>\n",
       "      <td>Yum! Brands</td>\n",
       "      <td>1041061</td>\n",
       "      <td>YUM</td>\n",
       "      <td>2022-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>ZBRA</td>\n",
       "      <td>Zebra Technologies</td>\n",
       "      <td>877212</td>\n",
       "      <td>ZBRA</td>\n",
       "      <td>2022-02-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ZBH</td>\n",
       "      <td>Zimmer Biomet</td>\n",
       "      <td>1136869</td>\n",
       "      <td>ZBH</td>\n",
       "      <td>2022-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>ZION</td>\n",
       "      <td>Zions Bancorporation</td>\n",
       "      <td>109380</td>\n",
       "      <td>0000109380</td>\n",
       "      <td>2022-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis</td>\n",
       "      <td>1555280</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>2022-02-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol              Security      CIK  truth_path filing_date\n",
       "0      MMM                    3M    66740         MMM  2022-02-09\n",
       "1      AOS           A. O. Smith    91142         AOS  2022-02-11\n",
       "2      ABT                Abbott     1800         ABT  2022-02-18\n",
       "3     ABBV                AbbVie  1551152        ABBV  2022-02-18\n",
       "4      ACN             Accenture  1467373         ACN  2022-10-12\n",
       "..     ...                   ...      ...         ...         ...\n",
       "498    YUM           Yum! Brands  1041061         YUM  2022-02-28\n",
       "499   ZBRA    Zebra Technologies   877212        ZBRA  2022-02-10\n",
       "500    ZBH         Zimmer Biomet  1136869         ZBH  2022-02-25\n",
       "501   ZION  Zions Bancorporation   109380  0000109380  2022-02-25\n",
       "502    ZTS                Zoetis  1555280         ZTS  2022-02-15\n",
       "\n",
       "[503 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the 2022 returns in a separate table\n",
    "sp500_rets = sp500_orig.copy()\n",
    "\n",
    "# Get the filing date for each 10-K\n",
    "session = HTMLSession()\n",
    "\n",
    "for i in tqdm(range(len(sp500_rets))):\n",
    "    tic = sp500_rets['Symbol'].iloc[i]\n",
    "    cik = sp500_rets['CIK'].iloc[i]\n",
    "    truth_path = sp500_rets['truth_path'].iloc[i]\n",
    "    \n",
    "    if not os.path.exists(fr'{firm_10k_path}/{truth_path}/10-K/'):\n",
    "        print(f'Error finding accession number for ticker {tic}, cik {cik}')\n",
    "        continue\n",
    "    accession = os.listdir(fr'{firm_10k_path}/{truth_path}/10-K/')[0]\n",
    "    \n",
    "    url = f'https://www.sec.gov/Archives/edgar/data/{cik}/{accession}-index.html'\n",
    "    r = session.get(url)\n",
    "    try:\n",
    "        sp500_rets.loc[i, 'filing_date'] = r.html.find('.info', first=True).text\n",
    "    except Exception as error:\n",
    "        print(f'Could not get filing date for ticker {tic}, cik {cik}, accession number {accession}: {repr(error)}')\n",
    "    sleep(0.1)\n",
    "\n",
    "sp500_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db8388d-726a-4d46-aeb5-01c7e2700566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>-0.011276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-02</td>\n",
       "      <td>0.030954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>0.000287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>0.014362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>0.012459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587061</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>-0.017551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587062</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>-0.114089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587063</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>0.033089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587064</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>0.080827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587065</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>0.011164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2587066 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ticker       date       ret\n",
       "0         JJSF 2021-12-01 -0.011276\n",
       "1         JJSF 2021-12-02  0.030954\n",
       "2         JJSF 2021-12-03  0.000287\n",
       "3         JJSF 2021-12-06  0.014362\n",
       "4         JJSF 2021-12-07  0.012459\n",
       "...        ...        ...       ...\n",
       "2587061   TSLA 2022-12-23 -0.017551\n",
       "2587062   TSLA 2022-12-27 -0.114089\n",
       "2587063   TSLA 2022-12-28  0.033089\n",
       "2587064   TSLA 2022-12-29  0.080827\n",
       "2587065   TSLA 2022-12-30  0.011164\n",
       "\n",
       "[2587066 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download 2022 CSRP returns\n",
    "url = \"https://github.com/LeDataSciFi/data/raw/main/Stock%20Returns%20(CRSP)/crsp_2022_only.zip\"\n",
    "with urlopen(url) as request:\n",
    "    data = BytesIO(request.read())\n",
    "\n",
    "with ZipFile(data) as archive:\n",
    "    with archive.open(archive.namelist()[0]) as stata:\n",
    "        stock_rets = pd.read_stata(stata)\n",
    "\n",
    "stock_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9717040-92bd-47cf-abbe-fd5dc8f6d222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>CIK</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>ret_t-t2</th>\n",
       "      <th>ret_t3-t10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>66740</td>\n",
       "      <td>2022-02-09</td>\n",
       "      <td>-0.017671</td>\n",
       "      <td>-0.090256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>91142</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>-0.014543</td>\n",
       "      <td>-0.138970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>1800</td>\n",
       "      <td>2022-02-18</td>\n",
       "      <td>-0.041758</td>\n",
       "      <td>-0.127147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2022-02-18</td>\n",
       "      <td>-0.029927</td>\n",
       "      <td>-0.111148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>1467373</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>-0.026583</td>\n",
       "      <td>-0.015658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>YUM</td>\n",
       "      <td>Yum! Brands</td>\n",
       "      <td>1041061</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>1.278068</td>\n",
       "      <td>-0.993568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>ZBRA</td>\n",
       "      <td>Zebra Technologies</td>\n",
       "      <td>877212</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>0.919714</td>\n",
       "      <td>-0.993694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>ZBH</td>\n",
       "      <td>Zimmer Biomet</td>\n",
       "      <td>1136869</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>0.969666</td>\n",
       "      <td>-0.993964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>ZION</td>\n",
       "      <td>Zions Bancorporation</td>\n",
       "      <td>109380</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>0.909961</td>\n",
       "      <td>-0.994032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis</td>\n",
       "      <td>1555280</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>0.877291</td>\n",
       "      <td>-0.993940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol              Security      CIK filing_date  ret_t-t2  ret_t3-t10\n",
       "0      MMM                    3M    66740  2022-02-09 -0.017671   -0.090256\n",
       "1      AOS           A. O. Smith    91142  2022-02-11 -0.014543   -0.138970\n",
       "2      ABT                Abbott     1800  2022-02-18 -0.041758   -0.127147\n",
       "3     ABBV                AbbVie  1551152  2022-02-18 -0.029927   -0.111148\n",
       "4      ACN             Accenture  1467373  2022-10-12 -0.026583   -0.015658\n",
       "..     ...                   ...      ...         ...       ...         ...\n",
       "492    YUM           Yum! Brands  1041061  2022-02-28  1.278068   -0.993568\n",
       "493   ZBRA    Zebra Technologies   877212  2022-02-10  0.919714   -0.993694\n",
       "494    ZBH         Zimmer Biomet  1136869  2022-02-25  0.969666   -0.993964\n",
       "495   ZION  Zions Bancorporation   109380  2022-02-25  0.909961   -0.994032\n",
       "496    ZTS                Zoetis  1555280  2022-02-15  0.877291   -0.993940\n",
       "\n",
       "[497 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on filing date, add return from t to t+2 and from t+3 to t+10\n",
    "combined_rets = sp500_rets.merge(\n",
    "        stock_rets.rename(columns={'ticker':'Symbol'}),\n",
    "        on='Symbol',\n",
    "        how='left',\n",
    "        validate='1:m') # TODO: include more?\n",
    "combined_rets = combined_rets.query('filing_date <= date').groupby('Symbol').head(11)\n",
    "combined_rets['agg_ret'] = 1 + combined_rets['ret']\n",
    "\n",
    "combined_rets['ret_t-t2'] = combined_rets.groupby('Symbol') \\\n",
    "        .head(3)['agg_ret'] \\\n",
    "        .cumprod() - 1\n",
    "combined_rets['ret_t3-t10'] = combined_rets.groupby('Symbol') \\\n",
    "        .tail(8)['agg_ret'] \\\n",
    "        .cumprod() - 1\n",
    "final_rets = combined_rets.groupby('Symbol') \\\n",
    "        .head(3).groupby('Symbol') \\\n",
    "        .tail(1)[['Symbol', 'Security', 'CIK', 'filing_date', 'ret_t-t2']]\n",
    "final_rets = final_rets.merge(\n",
    "        combined_rets.groupby('Symbol') \\\n",
    "                .tail(1)[['Symbol', 'Security', 'CIK', 'ret_t3-t10']],\n",
    "        on=['Symbol', 'Security', 'CIK'],\n",
    "        validate='1:1',\n",
    "        how='left') # TODO: include more?\n",
    "\n",
    "final_rets.to_csv(returns_save_path, index=False)\n",
    "final_rets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba1f68-1387-4217-9abc-e582e61f1a56",
   "metadata": {},
   "source": [
    "## Clean 10-Ks\n",
    "\n",
    "For each 10-K, we remove xml and html tags, extracting only the text.  We make the entire document lowercase and keep only words. If this step has already been done, we skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62a37c5b-5e39-4162-98c0-6f06aef4a0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████▌                                          | 233/503 [00:00<00:00, 371.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker FRC\n",
      "Cannot find 10-K for ticker GEHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████▊   | 483/503 [00:01<00:00, 344.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker SBNY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 503/503 [00:01<00:00, 352.13it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(firm_10k_clean_path, exist_ok=True)\n",
    "sp500_sents = sp500_orig.copy()\n",
    "\n",
    "for i in tqdm(range(len(sp500_sents))):\n",
    "    tic = sp500_sents['Symbol'].iloc[i]\n",
    "    truth_path = sp500_sents['truth_path'].iloc[i]    # TODO: explain\n",
    "\n",
    "    # Check existence of path\n",
    "    if not os.path.exists(fr'{firm_10k_path}/{truth_path}'):\n",
    "        print(f'Cannot find 10-K for ticker {tic}')\n",
    "        continue\n",
    "    \n",
    "    # Create clean path\n",
    "    os.makedirs(fr'{firm_10k_clean_path}/{truth_path}', exist_ok=True)\n",
    "    \n",
    "    for path in glob.glob(fr'{firm_10k_path}/{truth_path}/*/*/*.html'):\n",
    "        # Check existence of cleaned 10-K\n",
    "        if os.path.exists(fr'{firm_10k_clean_path}/{truth_path}/10-K.txt'):\n",
    "            continue\n",
    "        \n",
    "        # Open and clean the 10-K\n",
    "        with open(path, 'rb') as report_file:\n",
    "            html = report_file.read()\n",
    "        soup = BeautifulSoup(html, 'lxml-xml')\n",
    "        for div in soup.find_all(\"div\", {'style': 'display:none'}):\n",
    "            div.dintlpose()                       # remove hidden divs,\n",
    "        lower = soup.get_text().lower()           # uppercase,\n",
    "        no_punc = re.sub(r'\\W', ' ', lower)       # non-alpha-numeric,\n",
    "        cleaned = re.sub(r'\\s+', ' ', no_punc)    # single-space\n",
    "        \n",
    "        # Persist changes to the clean directory\n",
    "        result_path = fr'{firm_10k_clean_path}/{truth_path}/10-K.txt' \n",
    "        with open(result_path, 'wb') as result_file:\n",
    "            result_file.write(cleaned.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d419bcd-216c-4760-a4fe-180b89c7c21f",
   "metadata": {},
   "source": [
    "## Load sentiment dictionaries\n",
    "\n",
    "Now, we load the sentiment dictionaries.\n",
    "\n",
    "For the LM dictionary, we only keep positive values.  The dictionary was released in 2021, and the year that any word was added to the list is included as a positive value.  The year a word was removed from the list, it is included as a negative value.  Any negative values have been removed from the list during some release, so we should only keep the positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8025a5e8-5384-4cc7-87ac-0ebdad9212af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Dictionaries\n",
    "with open('inputs/ML_negative_unigram.txt', 'r') as file:\n",
    "    BHR_negative = [line.strip() for line in file]\n",
    "with open('inputs/ML_positive_unigram.txt', 'r') as file:\n",
    "    BHR_positive = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8936ac5-2136-4730-8a72-c582d7d5c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM Dictionaries\n",
    "LM = pd.read_csv('inputs/LM_MasterDictionary_1993-2021.csv')\n",
    "LM_negative = LM.query('Negative > 0')['Word'].to_list()\n",
    "LM_positive = LM.query('Positive > 0')['Word'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc85e2a-0ec6-4231-bafd-a00dc9c71ec1",
   "metadata": {},
   "source": [
    "## Generate sentiment regex\n",
    "\n",
    "We start by gathering the sentiments into regex patterns.  We define a function to generate the NEAR_regex for each pair of two topics, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aab6e2a7-47f6-446d-8dbf-558eeb76bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather sentiments into regex\n",
    "BHR_negative_regex = '(' + '|'.join(BHR_negative).lower() + ')'\n",
    "BHR_positive_regex = '(' + '|'.join(BHR_positive).lower() + ')'\n",
    "LM_negative_regex = '(' + '|'.join(LM_negative).lower() + ')'\n",
    "LM_positive_regex = '(' + '|'.join(LM_positive).lower() + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "201d2cd9-291f-4bd2-bfb8-c3fe6e393b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic regex\n",
    "def NEAR_regex_helper(topic_df, topic_name, max_words_between=5):\n",
    "    topic_list = topic_df['term'].loc[topic_df['type'] == topic_name + '_topic']\n",
    "    positive_list = topic_df['term'].loc[topic_df['type'] == topic_name + '_positive']\n",
    "    negative_list = topic_df['term'].loc[topic_df['type'] == topic_name + '_negative']\n",
    "    \n",
    "    topic_regex = '(' + '|'.join(topic_list).lower() + ')'\n",
    "    positive_regex = '(' + '|'.join(positive_list).lower() + ')'\n",
    "    negative_regex = '(' + '|'.join(negative_list).lower() + ')'\n",
    "    \n",
    "    positive_regex = NEAR_regex([topic_regex, positive_regex], max_words_between=max_words_between)\n",
    "    negative_regex = NEAR_regex([topic_regex, negative_regex], max_words_between=max_words_between)\n",
    "    return [positive_regex, negative_regex]\n",
    "\n",
    "# def NEAR_regex_helper(topic_list, valence_list, max_words_between=5):\n",
    "#     # print(topic_list.apply(NEAR_regex, args=(5,True)))\n",
    "#     topic_regex = '(' + '|'.join(topic_list).lower() + ')'\n",
    "#     valence_regex = '(' + '|'.join(valence_list).lower() + ')'\n",
    "#     return NEAR_regex([topic_regex, valence_regex], max_words_between=max_words_between)\n",
    "\n",
    "# Read in file\n",
    "if not os.path.exists(topic_path):\n",
    "    print(f'Cannot find path {topic_path} to topic list')\n",
    "else:\n",
    "    topic_df = pd.read_csv(topic_path)\n",
    "    \n",
    "    esg_positive_regex, esg_negative_regex = NEAR_regex_helper(topic_df, 'esg')\n",
    "    intl_positive_regex, intl_negative_regex = NEAR_regex_helper(topic_df, 'intl')\n",
    "    fs_positive_regex, fs_negative_regex = NEAR_regex_helper(topic_df, 'fs')\n",
    "    \n",
    "    bio_positive_regex, bio_negative_regex = NEAR_regex_helper(topic_df, 'bio')\n",
    "    cg_positive_regex, cg_negative_regex = NEAR_regex_helper(topic_df, 'cg')\n",
    "    cs_positive_regex, cs_negative_regex = NEAR_regex_helper(topic_df, 'cs')\n",
    "    csat_positive_regex, csat_negative_regex = NEAR_regex_helper(topic_df, 'csat')\n",
    "    ecom_positive_regex, ecom_negative_regex = NEAR_regex_helper(topic_df, 'ecom')\n",
    "    esat_positive_regex, esat_negative_regex = NEAR_regex_helper(topic_df, 'esat')\n",
    "    fp_positive_regex, fp_negative_regex = NEAR_regex_helper(topic_df, 'fp')\n",
    "    fs_positive_regex, fs_negative_regex = NEAR_regex_helper(topic_df, 'fs')\n",
    "    ia_positive_regex, ia_negative_regex = NEAR_regex_helper(topic_df, 'ia')\n",
    "    re_positive_regex, re_negative_regex = NEAR_regex_helper(topic_df, 're')\n",
    "    sc_positive_regex, sc_negative_regex = NEAR_regex_helper(topic_df, 'sc')\n",
    "    tech_positive_regex, tech_negative_regex = NEAR_regex_helper(topic_df, 'tech')\n",
    "    \n",
    "# set(topic_df['type']) \n",
    "    \n",
    "#     # TODO: revert to only 3\n",
    "    \n",
    "#     # ESG\n",
    "#     esg_topic = topic_df['term'].loc[topic_df['type'] == 'esg_topic']\n",
    "#     esg_negative = topic_df['term'].loc[topic_df['type'] == 'esg_negative']\n",
    "#     esg_positive = topic_df['term'].loc[topic_df['type'] == 'esg_positive']\n",
    "\n",
    "#     # International\n",
    "#     intl_topic = topic_df['term'].loc[topic_df['type'] == 'intl_topic']\n",
    "#     intl_negative = topic_df['term'].loc[topic_df['type'] == 'intl_negative']\n",
    "#     intl_positive = topic_df['term'].loc[topic_df['type'] == 'intl_positive']\n",
    "\n",
    "#     # Financial stability\n",
    "#     fs_topic = topic_df['term'].loc[topic_df['type'] == 'fs_topic']\n",
    "#     fs_negative = topic_df['term'].loc[topic_df['type'] == 'fs_negative']\n",
    "#     fs_positive = topic_df['term'].loc[topic_df['type'] == 'fs_positive']\n",
    "\n",
    "#     # Generate regex\n",
    "#     esg_negative_regex = NEAR_regex_helper(esg_topic, esg_negative)\n",
    "#     esg_positive_regex = NEAR_regex_helper(esg_topic, esg_positive)\n",
    "#     intl_negative_regex = NEAR_regex_helper(intl_topic, intl_negative)\n",
    "#     intl_positive_regex = NEAR_regex_helper(intl_topic, intl_positive)\n",
    "#     fs_negative_regex = NEAR_regex_helper(fs_topic, fs_negative)\n",
    "#     fs_positive_regex = NEAR_regex_helper(fs_topic, fs_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ef2d5-3bbd-449a-be39-33743bf409f1",
   "metadata": {},
   "source": [
    "## Load each firm and add sentiment variables\n",
    "\n",
    "TODO: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4b9eb13-c23d-4798-98d2-0fb2720d8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify adding a value to a given row\n",
    "def add_sentiment(df, i, sentiment_name, search, text, word_count):\n",
    "    df.loc[i, sentiment_name] = len(re.findall(search, text)) / word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81201d43-c23b-4601-bade-b3eadad933cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████▎                                                | 197/503 [22:00<38:07,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find clean 10-K for ticker FRC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████▉                                              | 213/503 [23:28<25:16,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find clean 10-K for ticker GEHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████▎             | 417/503 [43:21<08:40,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find clean 10-K for ticker SBNY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 503/503 [51:25<00:00,  6.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>bhr_negative</th>\n",
       "      <th>bhr_positive</th>\n",
       "      <th>lm_negative</th>\n",
       "      <th>lm_positive</th>\n",
       "      <th>esg_negative</th>\n",
       "      <th>esg_positive</th>\n",
       "      <th>intl_negative</th>\n",
       "      <th>intl_positive</th>\n",
       "      <th>...</th>\n",
       "      <th>fp_negative</th>\n",
       "      <th>fp_positive</th>\n",
       "      <th>ia_negative</th>\n",
       "      <th>ia_positive</th>\n",
       "      <th>re_negative</th>\n",
       "      <th>re_positive</th>\n",
       "      <th>sc_negative</th>\n",
       "      <th>sc_positive</th>\n",
       "      <th>tech_negative</th>\n",
       "      <th>tech_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5425.424000</td>\n",
       "      <td>69504.586000</td>\n",
       "      <td>0.038030</td>\n",
       "      <td>0.039918</td>\n",
       "      <td>0.036081</td>\n",
       "      <td>0.014795</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1177.831997</td>\n",
       "      <td>28943.930805</td>\n",
       "      <td>0.004226</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>0.004679</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>460.000000</td>\n",
       "      <td>1575.000000</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.023693</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4767.250000</td>\n",
       "      <td>51384.750000</td>\n",
       "      <td>0.035457</td>\n",
       "      <td>0.036979</td>\n",
       "      <td>0.033086</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5324.500000</td>\n",
       "      <td>64916.500000</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.039824</td>\n",
       "      <td>0.035853</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5893.500000</td>\n",
       "      <td>80526.000000</td>\n",
       "      <td>0.040611</td>\n",
       "      <td>0.043145</td>\n",
       "      <td>0.038983</td>\n",
       "      <td>0.016067</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10469.000000</td>\n",
       "      <td>271760.000000</td>\n",
       "      <td>0.051376</td>\n",
       "      <td>0.056662</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>0.021395</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_word_count     word_count  bhr_negative  bhr_positive  \\\n",
       "count         500.000000     500.000000    500.000000    500.000000   \n",
       "mean         5425.424000   69504.586000      0.038030      0.039918   \n",
       "std          1177.831997   28943.930805      0.004226      0.005064   \n",
       "min           460.000000    1575.000000      0.015659      0.010931   \n",
       "25%          4767.250000   51384.750000      0.035457      0.036979   \n",
       "50%          5324.500000   64916.500000      0.038217      0.039824   \n",
       "75%          5893.500000   80526.000000      0.040611      0.043145   \n",
       "max         10469.000000  271760.000000      0.051376      0.056662   \n",
       "\n",
       "       lm_negative  lm_positive  esg_negative  esg_positive  intl_negative  \\\n",
       "count   500.000000   500.000000    500.000000    500.000000     500.000000   \n",
       "mean      0.036081     0.014795      0.000827      0.001332       0.000133   \n",
       "std       0.004679     0.002147      0.000664      0.000576       0.000112   \n",
       "min       0.023693     0.004394      0.000000      0.000295       0.000000   \n",
       "25%       0.033086     0.013601      0.000435      0.001026       0.000055   \n",
       "50%       0.035853     0.014822      0.000568      0.001195       0.000107   \n",
       "75%       0.038983     0.016067      0.000910      0.001456       0.000180   \n",
       "max       0.055229     0.021395      0.004063      0.004683       0.000837   \n",
       "\n",
       "       intl_positive  ...  fp_negative  fp_positive  ia_negative  ia_positive  \\\n",
       "count     500.000000  ...   500.000000   500.000000   500.000000   500.000000   \n",
       "mean        0.000338  ...     0.000562     0.000671     0.000018     0.000051   \n",
       "std         0.000216  ...     0.000229     0.000283     0.000025     0.000055   \n",
       "min         0.000000  ...     0.000000     0.000000     0.000000     0.000000   \n",
       "25%         0.000177  ...     0.000400     0.000483     0.000000     0.000013   \n",
       "50%         0.000308  ...     0.000522     0.000624     0.000013     0.000036   \n",
       "75%         0.000448  ...     0.000681     0.000833     0.000024     0.000071   \n",
       "max         0.001453  ...     0.001421     0.001851     0.000172     0.000408   \n",
       "\n",
       "       re_negative  re_positive  sc_negative  sc_positive  tech_negative  \\\n",
       "count   500.000000   500.000000   500.000000   500.000000     500.000000   \n",
       "mean      0.000009     0.000008     0.002397     0.000015       0.000012   \n",
       "std       0.000014     0.000014     0.000889     0.000022       0.000045   \n",
       "min       0.000000     0.000000     0.000161     0.000000       0.000000   \n",
       "25%       0.000000     0.000000     0.001794     0.000000       0.000000   \n",
       "50%       0.000000     0.000000     0.002320     0.000010       0.000000   \n",
       "75%       0.000015     0.000013     0.002906     0.000021       0.000016   \n",
       "max       0.000109     0.000094     0.006602     0.000169       0.000953   \n",
       "\n",
       "       tech_positive  \n",
       "count     500.000000  \n",
       "mean        0.000047  \n",
       "std         0.000054  \n",
       "min         0.000000  \n",
       "25%         0.000014  \n",
       "50%         0.000035  \n",
       "75%         0.000064  \n",
       "max         0.000667  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and get the sentiment of clean 10-Ks\n",
    "temp_path = output_dir + '/ticker_sentiments_temp_inter.csv'\n",
    "\n",
    "# Compute sentiment ariables\n",
    "if not os.path.exists(sentiment_save_path):\n",
    "    for i in tqdm(range(len(sp500_sents))):\n",
    "        tic = sp500_sents['Symbol'].iloc[i]\n",
    "        truth_path = sp500_sents['truth_path'].iloc[i]    # TODO: explain\n",
    "\n",
    "        # Check existence of path\n",
    "        if not os.path.exists(fr'{firm_10k_clean_path}/{truth_path}'):\n",
    "            print(f'Cannot find clean 10-K for ticker {tic}')\n",
    "            continue\n",
    "\n",
    "        for path in glob.glob(fr'{firm_10k_clean_path}/{truth_path}/*.txt'):\n",
    "            with open(path, 'rb') as report_file:\n",
    "                cleaned = str(report_file.read())\n",
    "\n",
    "            # Add word count and unique word count\n",
    "            word_list = re.findall(r'\\w+', cleaned)\n",
    "            sp500_sents.loc[i, 'unique_word_count'] = len(set(word_list))\n",
    "            word_count = len(word_list)\n",
    "            sp500_sents.loc[i, 'word_count'] = word_count\n",
    "\n",
    "            # Gather valence variables\n",
    "            add_sentiment(sp500_sents, i, 'bhr_negative', BHR_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'bhr_positive', BHR_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'lm_negative', LM_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'lm_positive', LM_positive_regex, cleaned, word_count)\n",
    "\n",
    "            # TODO: revert to only 3\n",
    "            # Gather topic valence variables\n",
    "            add_sentiment(sp500_sents, i, 'esg_negative', esg_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'esg_positive', esg_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'intl_negative', intl_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'intl_positive', intl_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'fs_negative', fs_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'fs_positive', fs_positive_regex, cleaned, word_count)\n",
    "            \n",
    "            add_sentiment(sp500_sents, i, 'bio_negative', bio_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'bio_positive', bio_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'cg_negative', cg_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'cg_positive', cg_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'cs_negative', cs_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'cs_positive', cs_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'csat_negative', csat_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'csat_positive', csat_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'ecom_negative', ecom_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'ecom_positive', ecom_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'esat_negative', esat_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'esat_positive', esat_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'fp_negative', fp_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'fp_positive', fp_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'fs_negative', fs_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'fs_positive', fs_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'ia_negative', ia_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'ia_positive', ia_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 're_negative', re_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 're_positive', re_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'sc_negative', sc_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'sc_positive', sc_positive_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'tech_negative', tech_negative_regex, cleaned, word_count)\n",
    "            add_sentiment(sp500_sents, i, 'tech_positive', tech_positive_regex, cleaned, word_count)\n",
    "\n",
    "            # Save intermittently\n",
    "            if i % 50 == 0:\n",
    "                sp500_sents.to_csv(temp_path, index=False)\n",
    "\n",
    "    sp500_sents.to_csv(sentiment_save_path, index=False)\n",
    "else:\n",
    "    # Load existing sentiments\n",
    "    sp500_sents = pd.read_csv(sentiment_save_path)\n",
    "sp500_sents.drop(['CIK'], axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4685d86b-4e36-4871-aa3a-0f0a11942775",
   "metadata": {},
   "source": [
    "## Download 2021 CCM Data\n",
    "\n",
    "We download CCM data from GitHub to incorporate additional firm information into our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d29691e2-acc7-4588-bb85-0bd161b71d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CRSP dataframe\n",
    "crsp_2021_url = 'https://github.com/LeDataSciFi/data/raw/main/Firm%20Year%20Datasets%20(Compustat)/2021_ccm_cleaned.dta'\n",
    "crsp_2021 = pd.read_stata(crsp_2021_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3a77790-f2e0-4ca6-ab73-2ab72874e25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>CIK</th>\n",
       "      <th>truth_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>66740</td>\n",
       "      <td>MMM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>91142</td>\n",
       "      <td>AOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>1800</td>\n",
       "      <td>ABT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>1551152</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>1467373</td>\n",
       "      <td>ACN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol     Security      CIK truth_path\n",
       "0    MMM           3M    66740        MMM\n",
       "1    AOS  A. O. Smith    91142        AOS\n",
       "2    ABT       Abbott     1800        ABT\n",
       "3   ABBV       AbbVie  1551152       ABBV\n",
       "4    ACN    Accenture  1467373        ACN"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f652876-eee6-4dae-b968-015bb82a85b9",
   "metadata": {},
   "source": [
    "## Merge Sentiment Scores, Returns, and 2021 CCM Data\n",
    "\n",
    "Now, we merge our sets together and save the final dataframe for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d81be0c-7d07-41a4-a745-421794470c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>CIK</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>ret_t-t2</th>\n",
       "      <th>ret_t3-t10</th>\n",
       "      <th>truth_path</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>bhr_negative</th>\n",
       "      <th>...</th>\n",
       "      <th>mb</th>\n",
       "      <th>prof_a</th>\n",
       "      <th>ppe_a</th>\n",
       "      <th>cash_a</th>\n",
       "      <th>xrd_a</th>\n",
       "      <th>dltt_a</th>\n",
       "      <th>invopps_FG09</th>\n",
       "      <th>sales_g</th>\n",
       "      <th>dv_a</th>\n",
       "      <th>short_debt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>66740</td>\n",
       "      <td>2022-02-09</td>\n",
       "      <td>-0.017671</td>\n",
       "      <td>-0.090256</td>\n",
       "      <td>MMM</td>\n",
       "      <td>6384.0</td>\n",
       "      <td>76433.0</td>\n",
       "      <td>0.044313</td>\n",
       "      <td>...</td>\n",
       "      <td>2.838265</td>\n",
       "      <td>0.197931</td>\n",
       "      <td>0.218538</td>\n",
       "      <td>0.101228</td>\n",
       "      <td>0.042361</td>\n",
       "      <td>0.355625</td>\n",
       "      <td>2.564301</td>\n",
       "      <td>0.098527</td>\n",
       "      <td>0.072655</td>\n",
       "      <td>0.086095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>91142</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>-0.014543</td>\n",
       "      <td>-0.138970</td>\n",
       "      <td>AOS</td>\n",
       "      <td>3801.0</td>\n",
       "      <td>33811.0</td>\n",
       "      <td>0.032918</td>\n",
       "      <td>...</td>\n",
       "      <td>4.368153</td>\n",
       "      <td>0.197847</td>\n",
       "      <td>0.183974</td>\n",
       "      <td>0.181729</td>\n",
       "      <td>0.027113</td>\n",
       "      <td>0.061075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222291</td>\n",
       "      <td>0.048958</td>\n",
       "      <td>0.080191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>1800</td>\n",
       "      <td>2022-02-18</td>\n",
       "      <td>-0.041758</td>\n",
       "      <td>-0.127147</td>\n",
       "      <td>ABT</td>\n",
       "      <td>5047.0</td>\n",
       "      <td>52066.0</td>\n",
       "      <td>0.039488</td>\n",
       "      <td>...</td>\n",
       "      <td>3.825614</td>\n",
       "      <td>0.166285</td>\n",
       "      <td>0.134475</td>\n",
       "      <td>0.136297</td>\n",
       "      <td>0.036465</td>\n",
       "      <td>0.242726</td>\n",
       "      <td>3.559664</td>\n",
       "      <td>0.244654</td>\n",
       "      <td>0.042582</td>\n",
       "      <td>0.051893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2022-02-18</td>\n",
       "      <td>-0.029927</td>\n",
       "      <td>-0.111148</td>\n",
       "      <td>ABBV</td>\n",
       "      <td>5890.0</td>\n",
       "      <td>61568.0</td>\n",
       "      <td>0.035181</td>\n",
       "      <td>...</td>\n",
       "      <td>2.528878</td>\n",
       "      <td>0.194432</td>\n",
       "      <td>0.040074</td>\n",
       "      <td>0.067086</td>\n",
       "      <td>0.054911</td>\n",
       "      <td>0.442929</td>\n",
       "      <td>2.144449</td>\n",
       "      <td>0.227438</td>\n",
       "      <td>0.063203</td>\n",
       "      <td>0.163364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>1467373</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>-0.026583</td>\n",
       "      <td>-0.015658</td>\n",
       "      <td>ACN</td>\n",
       "      <td>5171.0</td>\n",
       "      <td>51961.0</td>\n",
       "      <td>0.034122</td>\n",
       "      <td>...</td>\n",
       "      <td>5.474851</td>\n",
       "      <td>0.195625</td>\n",
       "      <td>0.111674</td>\n",
       "      <td>0.189283</td>\n",
       "      <td>0.025902</td>\n",
       "      <td>0.063702</td>\n",
       "      <td>5.023477</td>\n",
       "      <td>0.140013</td>\n",
       "      <td>0.051790</td>\n",
       "      <td>0.215661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>BF.B</td>\n",
       "      <td>Brown–Forman</td>\n",
       "      <td>14693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000014693</td>\n",
       "      <td>4815.0</td>\n",
       "      <td>44204.0</td>\n",
       "      <td>0.036897</td>\n",
       "      <td>...</td>\n",
       "      <td>5.639590</td>\n",
       "      <td>0.210576</td>\n",
       "      <td>0.148909</td>\n",
       "      <td>0.137141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325279</td>\n",
       "      <td>5.401004</td>\n",
       "      <td>0.136377</td>\n",
       "      <td>0.130394</td>\n",
       "      <td>0.115614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>FRC</td>\n",
       "      <td>First Republic Bank</td>\n",
       "      <td>1132979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0001132979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>GEHC</td>\n",
       "      <td>GE HealthCare</td>\n",
       "      <td>1932393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0001932393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>SBNY</td>\n",
       "      <td>Signature Bank</td>\n",
       "      <td>1288784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0001288784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>WELL</td>\n",
       "      <td>Welltower</td>\n",
       "      <td>766704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>10469.0</td>\n",
       "      <td>110010.0</td>\n",
       "      <td>0.027870</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol             Security      CIK filing_date  ret_t-t2  ret_t3-t10  \\\n",
       "0      MMM                   3M    66740  2022-02-09 -0.017671   -0.090256   \n",
       "1      AOS          A. O. Smith    91142  2022-02-11 -0.014543   -0.138970   \n",
       "2      ABT               Abbott     1800  2022-02-18 -0.041758   -0.127147   \n",
       "3     ABBV               AbbVie  1551152  2022-02-18 -0.029927   -0.111148   \n",
       "4      ACN            Accenture  1467373  2022-10-12 -0.026583   -0.015658   \n",
       "..     ...                  ...      ...         ...       ...         ...   \n",
       "498   BF.B         Brown–Forman    14693         NaN       NaN         NaN   \n",
       "499    FRC  First Republic Bank  1132979         NaN       NaN         NaN   \n",
       "500   GEHC        GE HealthCare  1932393         NaN       NaN         NaN   \n",
       "501   SBNY       Signature Bank  1288784         NaN       NaN         NaN   \n",
       "502   WELL            Welltower   766704         NaN       NaN         NaN   \n",
       "\n",
       "     truth_path  unique_word_count  word_count  bhr_negative  ...        mb  \\\n",
       "0           MMM             6384.0     76433.0      0.044313  ...  2.838265   \n",
       "1           AOS             3801.0     33811.0      0.032918  ...  4.368153   \n",
       "2           ABT             5047.0     52066.0      0.039488  ...  3.825614   \n",
       "3          ABBV             5890.0     61568.0      0.035181  ...  2.528878   \n",
       "4           ACN             5171.0     51961.0      0.034122  ...  5.474851   \n",
       "..          ...                ...         ...           ...  ...       ...   \n",
       "498  0000014693             4815.0     44204.0      0.036897  ...  5.639590   \n",
       "499  0001132979                NaN         NaN           NaN  ...       NaN   \n",
       "500  0001932393                NaN         NaN           NaN  ...       NaN   \n",
       "501  0001288784                NaN         NaN           NaN  ...       NaN   \n",
       "502  0000766704            10469.0    110010.0      0.027870  ...       NaN   \n",
       "\n",
       "       prof_a     ppe_a    cash_a     xrd_a    dltt_a  invopps_FG09   sales_g  \\\n",
       "0    0.197931  0.218538  0.101228  0.042361  0.355625      2.564301  0.098527   \n",
       "1    0.197847  0.183974  0.181729  0.027113  0.061075           NaN  0.222291   \n",
       "2    0.166285  0.134475  0.136297  0.036465  0.242726      3.559664  0.244654   \n",
       "3    0.194432  0.040074  0.067086  0.054911  0.442929      2.144449  0.227438   \n",
       "4    0.195625  0.111674  0.189283  0.025902  0.063702      5.023477  0.140013   \n",
       "..        ...       ...       ...       ...       ...           ...       ...   \n",
       "498  0.210576  0.148909  0.137141  0.000000  0.325279      5.401004  0.136377   \n",
       "499       NaN       NaN       NaN       NaN       NaN           NaN       NaN   \n",
       "500       NaN       NaN       NaN       NaN       NaN           NaN       NaN   \n",
       "501       NaN       NaN       NaN       NaN       NaN           NaN       NaN   \n",
       "502       NaN       NaN       NaN       NaN       NaN           NaN       NaN   \n",
       "\n",
       "         dv_a  short_debt  \n",
       "0    0.072655    0.086095  \n",
       "1    0.048958    0.080191  \n",
       "2    0.042582    0.051893  \n",
       "3    0.063203    0.163364  \n",
       "4    0.051790    0.215661  \n",
       "..        ...         ...  \n",
       "498  0.130394    0.115614  \n",
       "499       NaN         NaN  \n",
       "500       NaN         NaN  \n",
       "501       NaN         NaN  \n",
       "502       NaN         NaN  \n",
       "\n",
       "[503 rows x 108 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500_final = final_rets.merge(sp500_sents, on=['Symbol', 'Security', 'CIK'], validate='1:1', how='outer')\n",
    "sp500_final = sp500_final.merge(crsp_2021.rename(columns={'tic':'Symbol'}),\n",
    "        on='Symbol',\n",
    "        how='left',\n",
    "        validate='1:1') # TODO: include more?\n",
    "sp500_final.to_csv(final_save_path, index=False)\n",
    "sp500_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fa8ca-1854-4786-9863-976e06121509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031a6a2-39b8-4f47-bfbf-dc4e5c413282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb683fb-94ed-4147-bef2-f9717610b56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30d776-5afb-4942-b626-254d23d69416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098beaf9-9bbb-4430-aec1-b41c51885390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c5dc6-4e42-4d7a-9364-e8b12fb93414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
