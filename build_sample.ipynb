{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f53bea-fefb-478f-9ba0-640fe7437e3e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98a70a4a-16f5-4b0c-b81d-8959032ae55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fnmatch\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "# from time import sleep\n",
    "\n",
    "# Load 2022 returns\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "\n",
    "# import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from near_regex import NEAR_regex\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abfadaba-7f99-4f53-8278-18682dc448ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and file handling\n",
    "input_dir = 'inputs'\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Inputs\n",
    "topic_path = input_dir + '/topic_list.csv'\n",
    "sp500_path = input_dir + '/s&p500_2022.csv'\n",
    "firm_10k_path = \"10k_files/sec-edgar-filings\"\n",
    "\n",
    "# Outputs\n",
    "sentiment_save_path = output_dir + '/ticker_sentiments.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b45f5f-e4de-4d83-8ae1-014115d1b1b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load 2022 returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db8388d-726a-4d46-aeb5-01c7e2700566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>-0.011276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-02</td>\n",
       "      <td>0.030954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>0.000287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>0.014362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>0.012459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594044</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>-0.017551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594045</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>-0.114089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594046</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>0.033089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594047</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>0.080827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594048</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>0.011164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2594049 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ticker       date       ret\n",
       "0         JJSF 2021-12-01 -0.011276\n",
       "1         JJSF 2021-12-02  0.030954\n",
       "2         JJSF 2021-12-03  0.000287\n",
       "3         JJSF 2021-12-06  0.014362\n",
       "4         JJSF 2021-12-07  0.012459\n",
       "...        ...        ...       ...\n",
       "2594044   TSLA 2022-12-23 -0.017551\n",
       "2594045   TSLA 2022-12-27 -0.114089\n",
       "2594046   TSLA 2022-12-28  0.033089\n",
       "2594047   TSLA 2022-12-29  0.080827\n",
       "2594048   TSLA 2022-12-30  0.011164\n",
       "\n",
       "[2594049 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://github.com/LeDataSciFi/data/raw/main/Stock%20Returns%20(CRSP)/crsp_2022_only.zip\"\n",
    "with urlopen(url) as request:\n",
    "    data = BytesIO(request.read())\n",
    "\n",
    "with ZipFile(data) as archive:\n",
    "    with archive.open(archive.namelist()[0]) as stata:\n",
    "        stock_rets = pd.read_stata(stata)\n",
    "\n",
    "stock_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7279d-13d3-48b5-8d54-2bc08ba66f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** TODO make this generic at top for both\n",
    "# Get df of companies\n",
    "if not os.path.exists(sp500_path):\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    pd.read_html(url)[0].to_csv(sp500_path, index=False)  # [1] shows updates\n",
    "\n",
    "sp500 = pd.read_csv(sp500_path)[['Symbol', 'Security', 'CIK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb63301-4fa8-4f8d-94a5-a06e6671f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filing date for each 10-K\n",
    "session = HTMLSession()\n",
    "\n",
    "for i in tqdm(range(len(sp500))):  # TODO:\n",
    "    tic = sp500['Symbol'].iloc[i]\n",
    "    cik = sp500['CIK'].iloc[i]\n",
    "    \n",
    "    if not os.path.exists(fr'{firm_10k_path}/{tic}/10-K/'):\n",
    "        print(f'Error finding accession number for {tic}')\n",
    "        continue\n",
    "    accession = os.listdir(fr'{firm_10k_path}/{tic}/10-K/')[0]\n",
    "    \n",
    "    url = f'https://www.sec.gov/Archives/edgar/data/{cik}/{accession}-index.html'\n",
    "    r = session.get(url)\n",
    "    sp500.loc[i, 'filing_date'] = r.html.find('.info', first=True).text\n",
    "\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9717040-92bd-47cf-abbe-fd5dc8f6d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on filing date, add return from t to t+2 and from t+3 to t+10\n",
    "combined_return = sp500.merge(\n",
    "        stock_rets.rename(columns={'ticker':'Symbol'}),\n",
    "        on='Symbol',\n",
    "        how='left',\n",
    "        validate='1:m') # TODO: include more?\n",
    "combined_return = combined_return.query('filing_date <= date').groupby('Symbol').head(11)\n",
    "combined_return['agg_ret'] = 1 + combined_return['ret']\n",
    "\n",
    "combined_return['prod2'] = combined_return.groupby('Symbol').head(3)['agg_ret'].cumprod() - 1\n",
    "combined_return['prod10'] = combined_return.groupby('Symbol').tail(8)['agg_ret'].cumprod() - 1\n",
    "final_return = combined_return.groupby('Symbol').head(3).groupby('Symbol').tail(1)[['Symbol', 'Security', 'CIK', 'filing_date', 'prod2']]\n",
    "final_return = final_return.merge(\n",
    "        combined_return.groupby('Symbol').tail(1)[['Symbol', 'Security', 'CIK', 'prod10']],\n",
    "        on=['Symbol', 'Security', 'CIK'],\n",
    "        validate='1:1',\n",
    "        how='left') # TODO: include more?\n",
    "\n",
    "final_return.to_csv(returns_save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d419bcd-216c-4760-a4fe-180b89c7c21f",
   "metadata": {},
   "source": [
    "## Load sentiment dictionaries\n",
    "\n",
    "TODO: justify that positive values as of 2021 are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8025a5e8-5384-4cc7-87ac-0ebdad9212af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Dictionaries\n",
    "with open('inputs/ML_negative_unigram.txt', 'r') as file:\n",
    "    BHR_negative = [line.strip() for line in file]\n",
    "with open('inputs/ML_positive_unigram.txt', 'r') as file:\n",
    "    BHR_positive = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8936ac5-2136-4730-8a72-c582d7d5c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM Dictionaries\n",
    "LM = pd.read_csv('inputs/LM_MasterDictionary_1993-2021.csv')\n",
    "LM_negative = LM.query('Negative > 0')['Word'].to_list()\n",
    "LM_positive = LM.query('Positive > 0')['Word'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc85e2a-0ec6-4231-bafd-a00dc9c71ec1",
   "metadata": {},
   "source": [
    "## Load each firm and add sentiment variables\n",
    "\n",
    "TODO: load and clean\n",
    "\n",
    "For each firm, \n",
    "\n",
    "- [ ] load the corresponding 10-K. Clean the text.\n",
    "\n",
    "- [ ] Create the sentiment measurements, and save those new measurements to the correct row and column in the dataframe.\n",
    "\n",
    "- [ ] Bonus: Save the total length of the document (# of words)\n",
    "\n",
    "- [ ] Bonus: Save the # of unique words (similar to total length)\n",
    "\n",
    "- [ ] Calculate returns from t to t+2 inclusive\n",
    "\n",
    "- [ ] Calculate returns from t+3 to t+10 inclusive\n",
    "\n",
    "- [ ] Download 2021 accounting data (2021 ccm_cleaned.dta) from the data repo (possibly useful in analysis) and add to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab6e2a7-47f6-446d-8dbf-558eeb76bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather sentiments into regex\n",
    "BHR_negative_regex = '(' + '|'.join(BHR_negative).lower() + ')'\n",
    "BHR_positive_regex = '(' + '|'.join(BHR_positive).lower() + ')'\n",
    "LM_negative_regex = '(' + '|'.join(LM_negative).lower() + ')'\n",
    "LM_positive_regex = '(' + '|'.join(LM_positive).lower() + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ecbb34-6075-4967-9f6a-45afece8fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Topic regex\n",
    "# def NEAR_regex_helper(topic_list, valence_list, max_words_between=5):\n",
    "#     topic_regex = '(' + '|'.join(topic_list).lower() + ')'\n",
    "#     valence_regex = '(' + '|'.join(valence_list).lower() + ')'\n",
    "#     return NEAR_regex([topic_regex, valence_regex], max_words_between=max_words_between)\n",
    "\n",
    "# # Socially responsible investing\n",
    "# esg_topics = ['esg', 'sustainable', 'sustainability', 'impact invest',\n",
    "#               'clean energy', 'gender', 'diversity', 'inclusion',\n",
    "#               'microfinance', 'ethical', 'cdfi']\n",
    "# esg_negative = ['limited', 'limit', 'underdeveloped', 'underdevelop',\n",
    "#                 'bureaucratic', 'slow', 'insufficient']\n",
    "# esg_positive = ['ethical', 'sustainable', 'profitable', 'profit',\n",
    "#                 'innovative', 'innovation', 'transformative', 'transform']\n",
    "# esg_negative_regex = NEAR_regex_helper(esg_topics, esg_negative)\n",
    "# esg_positive_regex = NEAR_regex_helper(esg_topics, esg_positive)\n",
    "\n",
    "# # Ecommerce\n",
    "# ecom_topics = ['online', 'digital payment', 'logistics', 'delivery', \n",
    "#                'mobile commerce', 'social commerce', 'dropship',\n",
    "#                'drop ship', 'social media']\n",
    "# ecom_negative = ['risky', 'unsustainable',  'unsustained', 'monopoly',\n",
    "#                  'monopolistic', 'unethical']\n",
    "# ecom_positive = ['convenienent', 'convenienence', 'accessible', 'access',\n",
    "#                  'innovative', 'innovation', 'profitable', 'profit',\n",
    "#                  'efficient']\n",
    "# ecom_negative_regex = NEAR_regex_helper(ecom_topics, ecom_negative)\n",
    "# ecom_positive_regex = NEAR_regex_helper(ecom_topics, ecom_positive)\n",
    "\n",
    "# # Biotech and healthcare\n",
    "# bio_topics = ['gene', 'biopharm', 'telemedic', 'personalized medic',\n",
    "#               'medical device', 'vaccine', 'precision medic', 'organ',\n",
    "#               'regenerative medic', 'prosthetic', 'clinic', 'fda',\n",
    "#               'health']\n",
    "# bio_negative = ['risky', 'expensive', 'slow', 'controversial', 'unethical']\n",
    "# bio_positive = ['new', 'safe', 'innovative', 'innovation',\n",
    "#                 'transformative', 'transform', 'life', 'lives']\n",
    "# bio_negative_regex = NEAR_regex_helper(bio_topics, bio_negative)\n",
    "# bio_positive_regex = NEAR_regex_helper(bio_topics, bio_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "201d2cd9-291f-4bd2-bfb8-c3fe6e393b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic regex\n",
    "def NEAR_regex_helper(topic_list, valence_list, max_words_between=5):\n",
    "    topic_regex = '(' + '|'.join(topic_list).lower() + ')'\n",
    "    valence_regex = '(' + '|'.join(valence_list).lower() + ')'\n",
    "    return NEAR_regex([topic_regex, valence_regex], max_words_between=max_words_between)\n",
    "\n",
    "# Read in file\n",
    "if not os.path.exists(topic_path):\n",
    "    print(f'Cannot find path {topic_path} to topic list')\n",
    "else:\n",
    "    topic_df = pd.read_csv(topic_path)\n",
    "\n",
    "    # ESG\n",
    "    esg_topic = topic_df['term'].loc[topic_df['type'] == 'esg_topic']\n",
    "    esg_negative = topic_df['term'].loc[topic_df['type'] == 'esg_negative']\n",
    "    esg_positive = topic_df['term'].loc[topic_df['type'] == 'esg_positive']\n",
    "\n",
    "    # Ecommerce\n",
    "    ecom_topic = topic_df['term'].loc[topic_df['type'] == 'ecom_topic']\n",
    "    ecom_negative = topic_df['term'].loc[topic_df['type'] == 'ecom_negative']\n",
    "    ecom_positive = topic_df['term'].loc[topic_df['type'] == 'ecom_positive']\n",
    "\n",
    "    # Bio\n",
    "    bio_topic = topic_df['term'].loc[topic_df['type'] == 'bio_topic']\n",
    "    bio_negative = topic_df['term'].loc[topic_df['type'] == 'bio_negative']\n",
    "    bio_positive = topic_df['term'].loc[topic_df['type'] == 'bio_positive']\n",
    "\n",
    "    # Generate regex\n",
    "    esg_negative_regex = NEAR_regex_helper(esg_topic, esg_negative)\n",
    "    esg_positive_regex = NEAR_regex_helper(esg_topic, esg_positive)\n",
    "    ecom_negative_regex = NEAR_regex_helper(ecom_topic, ecom_negative)\n",
    "    ecom_positive_regex = NEAR_regex_helper(ecom_topic, ecom_positive)\n",
    "    bio_negative_regex = NEAR_regex_helper(bio_topic, bio_negative)\n",
    "    bio_positive_regex = NEAR_regex_helper(bio_topic, bio_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1a8b55e-82d0-4270-a13e-d4dec142060c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?:\\\\b(esg|sustainable|sustained|sustainability|impact investing|impact invest|clean energy|clean investing|clean invest|diversity|equity|inclusion|microfinance|ethics|ethically|cdfi)\\\\b(?: +[^ \\\\n]*){0,5} *\\\\b(limit|limited|underdevelop|underdeveloped|bureaucratic|slow|slowly|insufficient|behind|poor|faulty|behind)\\\\b)|(?:\\\\b(limit|limited|underdevelop|underdeveloped|bureaucratic|slow|slowly|insufficient|behind|poor|faulty|behind)\\\\b(?: +[^ \\\\n]*){0,5} *\\\\b(esg|sustainable|sustained|sustainability|impact investing|impact invest|clean energy|clean investing|clean invest|diversity|equity|inclusion|microfinance|ethics|ethically|cdfi)\\\\b)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esg_negative_regex\n",
    "# # esg_positive_regex\n",
    "# # ecom_negative_regex\n",
    "# # ecom_positive_regex\n",
    "# # bio_negative_regex\n",
    "# # bio_positive_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be15445f-5c9e-40a6-be0a-16015841d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(sp500_path):\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    pd.read_html(url)[0].to_csv(sp500_path, index=False)  # [1] shows updates\n",
    "\n",
    "sp500 = pd.read_csv(sp500_path)[['Symbol', 'Security', 'CIK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4b9eb13-c23d-4798-98d2-0fb2720d8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment(df, i, sentiment_name, search, text, word_count):\n",
    "    df.loc[i, sentiment_name] = len(re.findall(search, text)) / word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81201d43-c23b-4601-bade-b3eadad933cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████▊                                 | 64/503 [14:43<1:42:36, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker BRK.B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████                                | 80/503 [18:04<1:37:50, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker BF.B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▍                        | 169/503 [38:07<1:17:33, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker ELV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████▍                      | 197/503 [45:40<1:35:45, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker FRC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████████▋                     | 213/503 [48:56<1:04:28, 13.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find 10-K for ticker GEHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████▏          | 356/503 [1:19:36<20:09,  8.23s/it]"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(sp500))):\n",
    "    tic = sp500['Symbol'].iloc[i]\n",
    "    \n",
    "    # Check existence of path\n",
    "    if not os.path.exists(fr'{firm_10k_path}/{tic}'):\n",
    "        print(f'Cannot find 10-K for ticker {tic}')\n",
    "        continue\n",
    "    \n",
    "    for path in glob.glob(fr'{firm_10k_path}/{tic}/*/*/*.html'):\n",
    "        # Open and clean the 10-K\n",
    "        with open(path, 'rb') as report_file:\n",
    "            html = report_file.read()\n",
    "        soup = BeautifulSoup(html, 'lxml-xml')\n",
    "        for div in soup.find_all(\"div\", {'style': 'display:none'}):\n",
    "            div.decompose()\n",
    "        lower = soup.get_text().lower()\n",
    "        no_punc = re.sub(r'\\W', ' ', lower)\n",
    "        cleaned = re.sub(r'\\s+', ' ', no_punc)\n",
    "    \n",
    "        # Add word count and unique word count\n",
    "        word_list = re.findall(r'\\w+', cleaned)\n",
    "        sp500.loc[i, 'unique_word_count'] = len(set(word_list))\n",
    "        word_count = len(word_list)\n",
    "        sp500.loc[i, 'word_count'] = word_count\n",
    "        \n",
    "        # Gather valence variables\n",
    "        add_sentiment(sp500, i, 'bhr_negative', BHR_negative_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'bhr_positive', BHR_positive_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'lm_negative', LM_negative_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'lm_positive', LM_positive_regex, cleaned, word_count)\n",
    "        \n",
    "        # Gather topic valence variables\n",
    "        add_sentiment(sp500, i, 'esg_negative', esg_negative_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'esg_positive', esg_positive_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'ecom_negative', ecom_negative_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'ecom_positive', ecom_positive_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'bio_negative', bio_negative_regex, cleaned, word_count)\n",
    "        add_sentiment(sp500, i, 'bio_positive', bio_positive_regex, cleaned, word_count)\n",
    "        \n",
    "        # Save intermittently\n",
    "        if i % 25 == 0:\n",
    "            sp500.to_csv(sentiment_save_path, index=False)\n",
    "\n",
    "sp500.to_csv(sentiment_save_path, index=False)\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89391a-1bf8-40a1-88b7-6b2efe8e9cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4685d86b-4e36-4871-aa3a-0f0a11942775",
   "metadata": {},
   "source": [
    "## Add 10-K sentiment data\n",
    "\n",
    "TODO: load and clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29691e2-acc7-4588-bb85-0bd161b71d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f481ec-45af-40e1-a139-e4b4800607ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100815c-41e1-405a-8a0f-e4d2964233f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
